This has been generated by the StormCrawler Maven Archetype as a starting point for building your own crawler with Elasticsearch as a backend.
Have a look at the code and resources and modify them to your heart's content. 

First generate an uberjar:

``` sh
mvn clean package
```

then with Elasticsearch running locally, run `./ES_IndexInit.sh` to define the indices used by StormCrawler.

The first step consists in creating a file _seeds.txt_ in the current directory and populating it with the URLs 
to be used as a starting point for the crawl, e.g. 

`echo "http://stormcrawler.net/" > seeds.txt`

You can start the crawl topology using the Java class

``` sh
storm local target/${artifactId}-${version}.jar --local-ttl 3600 ${package}.ESCrawlTopology -- -conf crawler-conf.yaml -conf es-conf.yaml . seeds.txt
```

This will run the topology in local mode for 1 hour, using the URLs in _seeds.txt_ as a starting point. To start the topology in distributed mode, where it will run indefinitely, launch it with 'storm jar'.

Alternatively, you can also use Flux to do the same:

``` sh
storm local target/${artifactId}-${version}.jar  org.apache.storm.flux.Flux es-crawler.flux --local-ttl 3600
```

Note that in local mode, Flux uses a default TTL for the topology of 20 secs. The command above runs the topology for 1 hour.


It is best to run the topology with `storm jar` to benefit from the Storm UI and logging. In that case, the topology runs continuously, as intended.

Kibana
---------------------

To import the dashboards into a local instance of Kibana, go into the folder _kibana_ and run the script _importKibana.sh_. 

You should see something like 

```
Importing status dashboard into Kibana
{"success":true,"successCount":4}
Importing metrics dashboard into Kibana
{"success":true,"successCount":9}
```

The [dashboard screen](http://localhost:5601/app/kibana#/dashboards) should show both the status and metrics dashboards. If you click on `Crawl Status`, you should see 2 tables containing the count of URLs per status and the top hostnames per URL count.
The [Metrics dashboard](http://localhost:5601/app/kibana#/dashboard/Crawl-metrics) can be used to monitor the progress of the crawl.

The file _storm.ndjson_ is used to display some of Storm's internal metrics and is not added by default.



Happy crawling! If you have any questions, please ask on [StackOverflow with the tag stormcrawler](http://stackoverflow.com/questions/tagged/stormcrawler). 



